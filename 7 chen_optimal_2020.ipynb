{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80758a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli, uniform\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ca4e7",
   "metadata": {},
   "source": [
    "## Tong Chen's Model\n",
    "https://github.com/T0ngChen/multiwave/blob/master/sim.r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5448bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dir = \"data/chen_optimal_2020/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "232f7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data from R\n",
    "x_vars = [\"x\", \"z1\", \"z2\"]\n",
    "data1 = pd.read_csv(r_dir+\"data1.csv\", index_col=0)\n",
    "dm = data1[x_vars]\n",
    "dm.insert(0, \"intercept\", [1]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dffc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X=data1[x_vars], y=data1[\"y\"])\n",
    "fitted_values = model.predict_proba(data1[x_vars])[:,1]\n",
    "resid = data1[\"y\"] - fitted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8ba7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating influence\n",
    "Ihat = (dm.T*fitted_values*(1-fitted_values))@dm/len(dm)\n",
    "infl = (dm.T*resid).T@np.linalg.inv(Ihat)\n",
    "infl.columns = [\"infl_intercept\"] + [\"infl_\"+ x for x in x_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b0e5081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     693.098742\n",
       "1    1166.974278\n",
       "2     558.806858\n",
       "3     846.160103\n",
       "4     441.229744\n",
       "5     635.172133\n",
       "6     311.252347\n",
       "7     583.062125\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating optimal allocation for stratas\n",
    "aa = pd.concat([data1, infl], axis=\"columns\")\n",
    "std = (aa.groupby(\"stra\")\n",
    "         .agg({\"infl_x\":\"std\"})\n",
    "         .rename(columns={\"infl_x\":\"std\"})\n",
    "         .reset_index())\n",
    "n = (aa.groupby(\"stra\")\n",
    "       .agg({\"infl_x\":\"count\"})\n",
    "       .rename(columns={\"infl_x\":\"count\"})\n",
    "       .reset_index())\n",
    "oa = pd.merge(std, n, on=\"stra\")\n",
    "NS = oa[\"std\"]*oa[\"count\"]\n",
    "NS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12016b",
   "metadata": {},
   "source": [
    "## Intrusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff999dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/anaconda3/envs/bayes_topic_measures/lib/python3.10/site-packages/pymc/sampling_jax.py:37: UserWarning: This module is experimental.\n",
      "  warnings.warn(\"This module is experimental.\")\n"
     ]
    }
   ],
   "source": [
    "from getopt import getopt\n",
    "import cloudpickle\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from scipy.special import logit, expit\n",
    "from scipy.stats import uniform, norm, bernoulli, betabinom\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# from matplotlib import pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from modeltools import mcmc_diagnostics, create_summary_stat\n",
    "from downcast import downcast_df\n",
    "import jax\n",
    "from pymc.sampling_jax import sample_numpyro_nuts\n",
    "from time import time, sleep\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592363fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746640e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "process_n=None\n",
    "n_runs=30\n",
    "trials_per_sim=1\n",
    "optimal_allocation=False\n",
    "seed=42\n",
    "sim_name=\"7_topic_allocation\"\n",
    "chain_method = \"vectorized\"\n",
    "\n",
    "SAMPLE_JAX = True\n",
    "N_PROCESSES = 6\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# simulate_scores\n",
    "sim_id=0\n",
    "p_diff=0.08\n",
    "n_raters=40\n",
    "scores_per_r=40\n",
    "total_scores=None\n",
    "trials_per_sim=1\n",
    "seed=42\n",
    "optimal_allocation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2a7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(all_ids, param, size, bound=0.1):\n",
    "    # resampling raters and topics such that effects sum to 0.\n",
    "\n",
    "    s = model[\"summary_stat\"][model[\"summary_stat\"][\"param\"]==param].copy(deep=True)\n",
    "\n",
    "    if param == \"za\":\n",
    "        s[[\"a\", \"b\"]] = s[\"param_num\"].str.split(\", \", expand=True)\n",
    "        s[\"param_num\"] = (s[\"a\"].astype(int)*50 + s[\"b\"].astype(int)).astype(str)\n",
    "\n",
    "    mean_sum = 9999\n",
    "    while mean_sum < -bound or mean_sum > bound:\n",
    "        ids = np.random.choice(all_ids, size=size, replace=True)\n",
    "        mean_sum = sum([s[(s[\"param_num\"]==str(i))][\"mean\"].item() for i in ids])\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056cb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "raw_data = pd.read_csv(\"data/unit_level_ratings.csv\",index_col = 0)\n",
    "raw_data = raw_data.sort_values(by=[\"corpus\", \"model\", \"topic\"])\n",
    "\n",
    "# Creating identifier for each corpus, model, and topic\n",
    "# Identifier is unique for topic \n",
    "corpus_ids = (raw_data.groupby([\"corpus\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "corpus_ids[\"corpus_id\"] = corpus_ids.index\n",
    "\n",
    "model_ids = (raw_data.groupby([\"model\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "model_ids[\"model_id\"] = model_ids.index\n",
    "\n",
    "cordel_ids = (raw_data.groupby([\"corpus\", \"model\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "cordel_ids[\"cordel_id\"] = cordel_ids.index \n",
    "\n",
    "topic_ids = (raw_data.groupby([\"corpus\", \"model\", \"topic\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "topic_ids[\"topic_id\"] = topic_ids[\"topic\"].astype(np.int16)\n",
    "\n",
    "rater_ids = (raw_data.groupby([\"corpus\", \"rater\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "rater_ids[\"rater_id\"] = rater_ids.index \n",
    "\n",
    "d1 = pd.merge(raw_data, corpus_ids, on=[\"corpus\"], how=\"left\")\n",
    "d2 = pd.merge(d1, model_ids, on=[\"model\"], how=\"left\")\n",
    "d3 = pd.merge(d2, cordel_ids, on=[\"corpus\",\"model\"], how=\"left\")\n",
    "d4 = pd.merge(d3, rater_ids, on=[\"corpus\", \"rater\"], how=\"left\")\n",
    "data = pd.merge(d4, topic_ids, on=[\"corpus\", \"model\", \"topic\"], how=\"left\")\n",
    "data = data[[\"corpus_id\", \"model_id\", \"cordel_id\", \"topic_id\", \"rater_id\", \"intrusion\", \"confidence\"]]\n",
    "data, na_s = downcast_df(data)\n",
    "\n",
    "# Setting up numpy arrays for pymc\n",
    "corpus_array = np.array(data[\"corpus_id\"])\n",
    "n_corpora = data[\"corpus_id\"].nunique()\n",
    "\n",
    "model_array = np.array(data[\"model_id\"])\n",
    "n_models = data[\"model_id\"].nunique()\n",
    "\n",
    "cordel_array = np.array(data[\"cordel_id\"])\n",
    "n_cordels = data[\"cordel_id\"].nunique()\n",
    "\n",
    "topic_array = np.array([data[\"cordel_id\"], data[\"topic_id\"]])\n",
    "n_topics = data[\"topic_id\"].nunique()\n",
    "\n",
    "rater_array = np.array(data[\"rater_id\"])\n",
    "obs_n_raters = data[\"rater_id\"].nunique()\n",
    "\n",
    "score_array = np.array(data[\"intrusion\"])\n",
    "\n",
    "# Adding cordel id to topic_ids dataframe\n",
    "topic_cordel_ids = pd.merge(topic_ids, cordel_ids, on=[\"corpus\", \"model\"], how=\"left\")\n",
    "\n",
    "# Reading model\n",
    "with open(\"bayesian_model/glmm.pickle\", \"rb\") as f:\n",
    "    model = cloudpickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f239a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [s]\n",
      "INFO:pymc:Sampling: [s]\n"
     ]
    }
   ],
   "source": [
    "# Setting numpy seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Creating df schema\n",
    "ps_data = pd.DataFrame(columns=[\"trial_id\", \"sim_cordel_id\", \"sim_topic_id\", \"sim_rater_id\", \n",
    "                                \"cordel_id\", \"topic_id\", \"rater_id\"], dtype=np.int16)\n",
    "\n",
    "for trial_id in range(trials_per_sim):\n",
    "\n",
    "    # data template\n",
    "    sim_data = pd.DataFrame(columns=[\"trial_id\", \"cordel_id\", \"topic_id\", \"rater_id\"])  \n",
    "\n",
    "    # Raters in this simulation\n",
    "    raters = resample(data[\"rater_id\"].unique(), param=\"zr\", size=n_raters, bound=1)\n",
    "\n",
    "    # Topics in this simulation (topic_cordel_ids index values)\n",
    "    sim_topics_0 = resample(range(len(topic_cordel_ids)), param=\"za\", size=50, bound=1)\n",
    "    sim_topics_1 = resample(range(len(topic_cordel_ids)), param=\"za\", size=50, bound=1)\n",
    "    sim_topics = np.concatenate((sim_topics_0, sim_topics_1))\n",
    "\n",
    "    # Loop - used to contain uniform sampling algorithm could use cleanup\n",
    "    # Produces df containing cross product between raters and topics\n",
    "    for sim_rater_id, rater in enumerate(raters):\n",
    "        rated_topics = np.array(range(100))\n",
    "\n",
    "        rated_topics_idx = sim_topics[rated_topics]\n",
    "\n",
    "    #     Append topics to simulation\n",
    "        d=topic_cordel_ids.loc[rated_topics_idx, [\"topic_id\", \"cordel_id\"]]\n",
    "        d[\"sim_rater_id\"]=sim_rater_id\n",
    "        d[\"sim_topic_id\"]=rated_topics\n",
    "        d[\"rater_id\"]=rater\n",
    "\n",
    "        sim_data = pd.concat([sim_data, d], axis=\"rows\", ignore_index=True)\n",
    "\n",
    "#     Adding one topic/rater interaction into df\n",
    "    sim_data[\"trial_id\"] = trial_id\n",
    "    sim_data.loc[sim_data[\"sim_topic_id\"].isin(range(0,50)),[\"sim_cordel_id\"]] = 0\n",
    "    sim_data.loc[sim_data[\"sim_topic_id\"].isin(range(50,100)),[\"sim_cordel_id\"]] = 1\n",
    "#     sim_data = pd.merge(sim_data, topic_counts[[\"cordel_id\", \"topic_id\", \"sim_cordel_id\"]]\n",
    "#                         ,on=[\"cordel_id\", \"topic_id\"], how=\"left\")\n",
    "    sim_data=sim_data.astype(np.int16)\n",
    "\n",
    "#     Appending interaction to ds.\n",
    "    ps_data = pd.concat([ps_data, sim_data], ignore_index=True)\n",
    "\n",
    "#         print(f\"Completed simulating topic/rater interactions in {time() - startt:.2f}s\")\n",
    "\n",
    "#     Simulating Scores\n",
    "pymc_model = model[\"model\"]\n",
    "trace = model[\"trace\"].copy()\n",
    "\n",
    "# Calculating proposed logodds means\n",
    "# model1 = model0 + p\n",
    "# https://www.wolframalpha.com/input?i=solve+for+x+and+y%2C+x%2By%3Dc%2C+1%2F%281%2Be%5E-x%29-1%2F%281%2Be%5E-y%29%3Dp\n",
    "mean_model_logodds = model[\"summary_stat\"][model[\"summary_stat\"][\"param\"]==\"mu\"][\"mean\"].mean()\n",
    "c = 2*mean_model_logodds\n",
    "C = np.exp(-c)\n",
    "det = p_diff**2-2*C*(p_diff**2-2)+(C**2)*(p_diff**2)\n",
    "quad = (-p_diff*(C+1)+det**0.5)/(2*(p_diff+1))\n",
    "proposed_model1_mean = -np.log(quad)\n",
    "proposed_model0_mean = c-proposed_model1_mean\n",
    "\n",
    "# Setting trace of cordel 0 and cordel 1 to proposed values\n",
    "trace.posterior[\"mu\"].loc[dict(mu_dim_0=0)] = proposed_model0_mean\n",
    "trace.posterior[\"mu\"].loc[dict(mu_dim_0=1)] = proposed_model1_mean\n",
    "\n",
    "sim_scores = pd.DataFrame(columns=[\"trial_id\", \"sim_cordel_id\", \"sim_topic_id\", \"sim_rater_id\", \"cordel_id\", \"topic_id\", \"rater_id\", \"intrusion\", ]\n",
    "                   ,dtype=np.int16)\n",
    "\n",
    "# TODO: add chain options\n",
    "for trial_id in range(1):#trials_per_sim):\n",
    "    # Setting data containing rater/topic interaction\n",
    "    sim_data = ps_data[ps_data[\"trial_id\"]==trial_id]\n",
    "    sim_rater_array = np.array(sim_data[\"rater_id\"], dtype=int)\n",
    "    topic_array = np.array([sim_data[\"cordel_id\"], sim_data[\"topic_id\"]], dtype=int)\n",
    "    cordel_array = np.array(sim_data[\"sim_cordel_id\"], dtype=int)\n",
    "\n",
    "    # Running simulation\n",
    "    with pymc_model:\n",
    "        pm.set_data({\n",
    "            \"raters\":sim_rater_array, \n",
    "            \"topics\":topic_array, \n",
    "            \"cordels\":cordel_array})\n",
    "        postrr_sim=pm.sample_posterior_predictive(trace.posterior.sel(\n",
    "            {\"chain\":[0], \"draw\":[np.random.randint(trials_per_sim) if trials_per_sim==1 else trial_id]})\n",
    "            ,predictions=True, progressbar=False, random_seed=np.random.randint(2**20))\n",
    "\n",
    "    # Adding results to sim_scores\n",
    "    s = (postrr_sim.predictions.to_dataframe().reset_index()\n",
    "          .rename(columns={\"s\":\"intrusion\"}))\n",
    "    trial_sim_scores = pd.concat([sim_data.reset_index(drop=True)\n",
    "                                 ,s[\"intrusion\"]], axis=\"columns\").astype(np.int16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a924272",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc_vars = [\"sim_cordel_id\", \"sim_topic_id\", \"sim_rater_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bec87b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(trial_sim_scores[enc_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4dad1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=enc.transform(trial_sim_scores[enc_vars]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e2e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X=dm, y=trial_sim_scores[\"intrusion\"])\n",
    "fitted_values = reg.predict_proba(dm)[:,1]\n",
    "resid = np.array(trial_sim_scores[\"intrusion\"] - fitted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208a4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating influence\n",
    "Ihat = (dm.T*fitted_values*(1-fitted_values))@dm/len(dm)\n",
    "infl = (dm.T*resid).T@np.linalg.inv(Ihat)\n",
    "\n",
    "var_len = [len(x) for x in enc.categories_]\n",
    "col_index = []\n",
    "for var_cats in enc.categories_:\n",
    "    col_index += list(var_cats)\n",
    "    \n",
    "infl = pd.DataFrame(infl,columns = pd.MultiIndex.from_tuples(zip(\n",
    "    [\"ohe_cordel_id\"]*var_len[0]+\n",
    "    [\"ohe_topic_id\"]*var_len[1]+\n",
    "    [\"ohe_rater_id\"]*var_len[2],\n",
    "    col_index), names=[\"feature\", \"category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48d4a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "infl_c0 = infl[(\"ohe_cordel_id\", 0)].to_numpy().reshape(-1, 1)\n",
    "infl_c1 = infl[(\"ohe_cordel_id\", 1)].to_numpy().reshape(-1, 1)\n",
    "infl_t0 = infl[[(\"ohe_topic_id\", i) for i in range(0, 50)]].to_numpy()\n",
    "infl_t1 = infl[[(\"ohe_topic_id\", i) for i in range(50, 100)]].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca31d13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.86220718,  25.43954961, -45.73964839, ...,  -4.49847509,\n",
       "        40.6213066 , -25.08654023])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(infl_c0 + infl_t0).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "684d405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp0 = trial_sim_scores.copy(deep=True)\n",
    "temp0.columns = pd.MultiIndex.from_tuples(zip([\"label_encoding\"]*temp0.shape[1],\n",
    "    temp0.columns))\n",
    "temp = pd.concat([temp0, infl], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9f504ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(temp[\"ohe_topic_id\"].to_numpy().T+temp[(\"ohe_cordel_id\", 0)].to_numpy()).shape\n",
    "\n",
    "\n",
    "\n",
    "# Dead end, we can't transform the influence of a log-odds parameter to the influence of a probability parameter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10ff262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">label_encoding</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ohe_cordel_id</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">ohe_rater_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>sim_cordel_id</th>\n",
       "      <th>sim_topic_id</th>\n",
       "      <th>sim_rater_id</th>\n",
       "      <th>cordel_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>intrusion</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>7.645094</td>\n",
       "      <td>-1.029301</td>\n",
       "      <td>...</td>\n",
       "      <td>2.080327</td>\n",
       "      <td>3.836373</td>\n",
       "      <td>-13.883396</td>\n",
       "      <td>-0.907581</td>\n",
       "      <td>-5.279558</td>\n",
       "      <td>-1.675720</td>\n",
       "      <td>2.300096</td>\n",
       "      <td>-5.163627</td>\n",
       "      <td>-4.907581</td>\n",
       "      <td>-4.547696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.357252</td>\n",
       "      <td>-0.312478</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.336267</td>\n",
       "      <td>-1.673066</td>\n",
       "      <td>-20.357062</td>\n",
       "      <td>-8.009866</td>\n",
       "      <td>-9.178265</td>\n",
       "      <td>-5.999468</td>\n",
       "      <td>7.347729</td>\n",
       "      <td>-8.673066</td>\n",
       "      <td>-8.009866</td>\n",
       "      <td>-8.167868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.630155</td>\n",
       "      <td>-0.462208</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.170353</td>\n",
       "      <td>-3.277249</td>\n",
       "      <td>11.188266</td>\n",
       "      <td>-6.384146</td>\n",
       "      <td>-6.437595</td>\n",
       "      <td>-7.063456</td>\n",
       "      <td>-20.635868</td>\n",
       "      <td>0.722751</td>\n",
       "      <td>-13.384146</td>\n",
       "      <td>11.883096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.930544</td>\n",
       "      <td>0.099016</td>\n",
       "      <td>...</td>\n",
       "      <td>6.735246</td>\n",
       "      <td>-1.534959</td>\n",
       "      <td>14.114017</td>\n",
       "      <td>-8.805164</td>\n",
       "      <td>1.059734</td>\n",
       "      <td>9.005451</td>\n",
       "      <td>-28.913730</td>\n",
       "      <td>12.465041</td>\n",
       "      <td>-0.805164</td>\n",
       "      <td>10.870348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>13.091925</td>\n",
       "      <td>-0.784013</td>\n",
       "      <td>...</td>\n",
       "      <td>23.440650</td>\n",
       "      <td>33.310501</td>\n",
       "      <td>4.659758</td>\n",
       "      <td>31.180352</td>\n",
       "      <td>17.115278</td>\n",
       "      <td>21.570798</td>\n",
       "      <td>36.091392</td>\n",
       "      <td>17.310501</td>\n",
       "      <td>17.180352</td>\n",
       "      <td>-7.494276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.858385</td>\n",
       "      <td>-4.009840</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.767661</td>\n",
       "      <td>-2.295722</td>\n",
       "      <td>-0.296999</td>\n",
       "      <td>-2.823784</td>\n",
       "      <td>-1.462814</td>\n",
       "      <td>-1.128630</td>\n",
       "      <td>-0.377353</td>\n",
       "      <td>-2.045722</td>\n",
       "      <td>-1.212814</td>\n",
       "      <td>23.695341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>8.705613</td>\n",
       "      <td>34.355758</td>\n",
       "      <td>...</td>\n",
       "      <td>9.287716</td>\n",
       "      <td>10.970866</td>\n",
       "      <td>-2.454957</td>\n",
       "      <td>10.654017</td>\n",
       "      <td>4.495592</td>\n",
       "      <td>7.446141</td>\n",
       "      <td>0.871965</td>\n",
       "      <td>10.970866</td>\n",
       "      <td>2.495592</td>\n",
       "      <td>-139.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>2.136118</td>\n",
       "      <td>-1.500713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134833</td>\n",
       "      <td>-1.038499</td>\n",
       "      <td>0.895007</td>\n",
       "      <td>-1.192164</td>\n",
       "      <td>-0.393997</td>\n",
       "      <td>-0.433000</td>\n",
       "      <td>0.633494</td>\n",
       "      <td>-0.538499</td>\n",
       "      <td>-0.143997</td>\n",
       "      <td>19.746040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>13.138301</td>\n",
       "      <td>3.470259</td>\n",
       "      <td>...</td>\n",
       "      <td>10.538312</td>\n",
       "      <td>16.444860</td>\n",
       "      <td>-6.975673</td>\n",
       "      <td>18.351408</td>\n",
       "      <td>11.304682</td>\n",
       "      <td>9.585037</td>\n",
       "      <td>-2.994430</td>\n",
       "      <td>16.444860</td>\n",
       "      <td>7.304682</td>\n",
       "      <td>-199.498869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.648920</td>\n",
       "      <td>4.300211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314223</td>\n",
       "      <td>-2.613654</td>\n",
       "      <td>4.710900</td>\n",
       "      <td>-3.541531</td>\n",
       "      <td>-0.505470</td>\n",
       "      <td>-1.721839</td>\n",
       "      <td>2.953607</td>\n",
       "      <td>-1.613654</td>\n",
       "      <td>0.494530</td>\n",
       "      <td>69.658223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label_encoding                                                    \\\n",
       "           trial_id sim_cordel_id sim_topic_id sim_rater_id cordel_id   \n",
       "0                 0             0            0            0         5   \n",
       "1                 0             0            1            0         4   \n",
       "2                 0             0            2            0         1   \n",
       "3                 0             0            3            0         3   \n",
       "4                 0             0            4            0         2   \n",
       "...             ...           ...          ...          ...       ...   \n",
       "3995              0             1           95           39         5   \n",
       "3996              0             1           96           39         4   \n",
       "3997              0             1           97           39         1   \n",
       "3998              0             1           98           39         0   \n",
       "3999              0             1           99           39         2   \n",
       "\n",
       "                                 ohe_cordel_id             ... ohe_rater_id  \\\n",
       "     topic_id rater_id intrusion             0          1  ...           30   \n",
       "0          27      252         1      7.645094  -1.029301  ...     2.080327   \n",
       "1          46      252         1     -2.357252  -0.312478  ...    -4.336267   \n",
       "2          25      252         0     -2.630155  -0.462208  ...    -5.170353   \n",
       "3           3      252         0     -2.930544   0.099016  ...     6.735246   \n",
       "4          43      252         1     13.091925  -0.784013  ...    23.440650   \n",
       "...       ...      ...       ...           ...        ...  ...          ...   \n",
       "3995       13      194         1     -2.858385  -4.009840  ...    -1.767661   \n",
       "3996       35      194         0      8.705613  34.355758  ...     9.287716   \n",
       "3997       32      194         1      2.136118  -1.500713  ...    -0.134833   \n",
       "3998       41      194         0     13.138301   3.470259  ...    10.538312   \n",
       "3999        0      194         1     -2.648920   4.300211  ...     0.314223   \n",
       "\n",
       "                                                                        \\\n",
       "             31         32         33         34         35         36   \n",
       "0      3.836373 -13.883396  -0.907581  -5.279558  -1.675720   2.300096   \n",
       "1     -1.673066 -20.357062  -8.009866  -9.178265  -5.999468   7.347729   \n",
       "2     -3.277249  11.188266  -6.384146  -6.437595  -7.063456 -20.635868   \n",
       "3     -1.534959  14.114017  -8.805164   1.059734   9.005451 -28.913730   \n",
       "4     33.310501   4.659758  31.180352  17.115278  21.570798  36.091392   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3995  -2.295722  -0.296999  -2.823784  -1.462814  -1.128630  -0.377353   \n",
       "3996  10.970866  -2.454957  10.654017   4.495592   7.446141   0.871965   \n",
       "3997  -1.038499   0.895007  -1.192164  -0.393997  -0.433000   0.633494   \n",
       "3998  16.444860  -6.975673  18.351408  11.304682   9.585037  -2.994430   \n",
       "3999  -2.613654   4.710900  -3.541531  -0.505470  -1.721839   2.953607   \n",
       "\n",
       "                                        \n",
       "             37         38          39  \n",
       "0     -5.163627  -4.907581   -4.547696  \n",
       "1     -8.673066  -8.009866   -8.167868  \n",
       "2      0.722751 -13.384146   11.883096  \n",
       "3     12.465041  -0.805164   10.870348  \n",
       "4     17.310501  17.180352   -7.494276  \n",
       "...         ...        ...         ...  \n",
       "3995  -2.045722  -1.212814   23.695341  \n",
       "3996  10.970866   2.495592 -139.009900  \n",
       "3997  -0.538499  -0.143997   19.746040  \n",
       "3998  16.444860   7.304682 -199.498869  \n",
       "3999  -1.613654   0.494530   69.658223  \n",
       "\n",
       "[4000 rows x 150 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[(\"estimator\", \"cordel0\")] = temp[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "393e10f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['infl_x'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m temp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([trial_sim_scores, infl], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m sd \u001b[38;5;241m=\u001b[39m (\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msim_topic_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfl_x\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m          \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfl_x\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msd\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      5\u001b[0m          \u001b[38;5;241m.\u001b[39mreset_index())\n\u001b[1;32m      6\u001b[0m n \u001b[38;5;241m=\u001b[39m (temp\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim_topic_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m        \u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfl_x\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      8\u001b[0m        \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfl_x\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      9\u001b[0m        \u001b[38;5;241m.\u001b[39mreset_index())\n\u001b[1;32m     10\u001b[0m oa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(sd, n, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim_topic_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayes_topic_measures/lib/python3.10/site-packages/pandas/core/groupby/generic.py:894\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m    893\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args, kwargs)\n\u001b[0;32m--> 894\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/bayes_topic_measures/lib/python3.10/site-packages/pandas/core/apply.py:169\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(arg):\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m~/anaconda3/envs/bayes_topic_measures/lib/python3.10/site-packages/pandas/core/apply.py:478\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     selected_obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selected_obj\n\u001b[1;32m    476\u001b[0m     selection \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selection\n\u001b[0;32m--> 478\u001b[0m arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     colg \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_gotitem(selection, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bayes_topic_measures/lib/python3.10/site-packages/pandas/core/apply.py:601\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m         cols_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(safe_sort(\u001b[38;5;28mlist\u001b[39m(cols)))\n\u001b[0;32m--> 601\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcols_sorted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['infl_x'] do not exist\""
     ]
    }
   ],
   "source": [
    "\n",
    "sd = (temp.groupby(\"sim_topic_id\")\n",
    "         .agg({\"infl_x\":\"std\"})\n",
    "         .rename(columns={\"infl_x\":\"sd\"})\n",
    "         .reset_index())\n",
    "n = (temp.groupby(\"sim_topic_id\")\n",
    "       .agg({\"infl_x\":\"count\"})\n",
    "       .rename(columns={\"infl_x\":\"count\"})\n",
    "       .reset_index())\n",
    "oa = pd.merge(sd, n, on=\"sim_topic_id\")\n",
    "NS = oa[\"sd\"]*oa[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce34980d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1], dtype=int16),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "       dtype=int16),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39], dtype=int16)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimal_allocation:\n",
    "    # Optimal topic allocation scores\n",
    "    scores = trial_sim_scores[:0]\n",
    "\n",
    "    if total_scores == None:\n",
    "        total_scores = n_raters*scores_per_r\n",
    "\n",
    "    # Allocate topics for each rater\n",
    "    for sim_rater_id in range(n_raters):\n",
    "        # Checking if all scores have been allocated\n",
    "        if total_scores <= 0:\n",
    "            break\n",
    "\n",
    "        # Calculating variance for each topic's posterior distribution\n",
    "        s = (scores.groupby(\"sim_topic_id\").agg({\"intrusion\":\"sum\"})\n",
    "             .rename(columns={\"intrusion\":\"sum\"}).reset_index())\n",
    "        c = (scores.groupby(\"sim_topic_id\").agg({\"intrusion\":\"count\"})\n",
    "             .rename(columns={\"intrusion\":\"count\"}).reset_index())\n",
    "        topic_var = pd.merge(s, c, on=\"sim_topic_id\")\n",
    "\n",
    "        # Create df with zeros if no data exists\n",
    "        if len(topic_var) < 100:\n",
    "            missing_topic_ids = [i for i in range(100) if i not in np.array(topic_var[\"sim_topic_id\"])]\n",
    "            missings = pd.DataFrame({\"sim_topic_id\":missing_topic_ids\n",
    "                                      ,\"sum\":[0]*len(missing_topic_ids)\n",
    "                                      ,\"count\":[0]*len(missing_topic_ids)})\n",
    "            topic_var = pd.concat([topic_var, missings])\n",
    "\n",
    "        # Calculating reduction in variance\n",
    "        topic_var[\"variance\"] = postrr_var(topic_var[\"sum\"], topic_var[\"count\"])\n",
    "        topic_var[\"p\"] = postrr_p(topic_var[\"sum\"], topic_var[\"count\"])\n",
    "        topic_var[\"var0\"] = postrr_var(topic_var[\"sum\"], topic_var[\"count\"]+1)\n",
    "        topic_var[\"var1\"] = postrr_var(topic_var[\"sum\"]+1, topic_var[\"count\"]+1)\n",
    "        topic_var[\"expected_var\"] = topic_var[\"p\"]*topic_var[\"var1\"]+(1-topic_var[\"p\"])*topic_var[\"var0\"]\n",
    "        topic_var[\"var_diff\"] = topic_var[\"expected_var\"]-topic_var[\"variance\"]\n",
    "\n",
    "        # Allocating topics\n",
    "        allocated_topics = topic_var.sort_values(\"var_diff\", ascending=True)[:scores_per_r][\"sim_topic_id\"]\n",
    "        total_scores -= scores_per_r\n",
    "\n",
    "        selected_scores = trial_sim_scores[(trial_sim_scores[\"sim_rater_id\"]==sim_rater_id)&\n",
    "                            (trial_sim_scores[\"sim_topic_id\"].isin(allocated_topics))]\n",
    "        scores = pd.concat([scores, selected_scores])\n",
    "else:\n",
    "    # Running total of scores\n",
    "    counts = np.zeros(100)\n",
    "    scores = trial_sim_scores[:0]\n",
    "    for sim_rater_id, rater in enumerate(raters):\n",
    "        # Set the probability. Topics with fewer samples have higher probability\n",
    "        counts = counts-counts.min()+1\n",
    "        p = 1/counts**20\n",
    "        p = p/p.sum()\n",
    "\n",
    "        # Sample according to probability\n",
    "        allocated_topics = np.random.choice(range(100), size=scores_per_r, replace=False, p=p)\n",
    "        counts[allocated_topics] += 1\n",
    "\n",
    "        selected_scores = trial_sim_scores[(trial_sim_scores[\"sim_rater_id\"]==sim_rater_id)&\n",
    "                        (trial_sim_scores[\"sim_topic_id\"].isin(allocated_topics))]\n",
    "        scores = pd.concat([scores, selected_scores])\n",
    "\n",
    "sim_scores = pd.concat([sim_scores, scores], axis=\"index\", ignore_index=True)\n",
    "sim_scores.to_csv(f\"data/{sim_name}/score_{sim_id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal topic allocation scores\n",
    "scores = trial_sim_scores[:0]\n",
    "\n",
    "if total_scores == None:\n",
    "    total_scores = n_raters*scores_per_r\n",
    "\n",
    "# Allocate topics for each rater\n",
    "for sim_rater_id in range(n_raters):\n",
    "    # Checking if all scores have been allocated\n",
    "    if total_scores <= 0:\n",
    "        break\n",
    "\n",
    "    # Calculating variance for each topic's posterior distribution\n",
    "    s = (scores.groupby(\"sim_topic_id\").agg({\"intrusion\":\"sum\"})\n",
    "         .rename(columns={\"intrusion\":\"sum\"}).reset_index())\n",
    "    c = (scores.groupby(\"sim_topic_id\").agg({\"intrusion\":\"count\"})\n",
    "         .rename(columns={\"intrusion\":\"count\"}).reset_index())\n",
    "    topic_var = pd.merge(s, c, on=\"sim_topic_id\")\n",
    "\n",
    "    # Create df with zeros if no data exists\n",
    "    if len(topic_var) < 100:\n",
    "        missing_topic_ids = [i for i in range(100) if i not in np.array(topic_var[\"sim_topic_id\"])]\n",
    "        missings = pd.DataFrame({\"sim_topic_id\":missing_topic_ids\n",
    "                                  ,\"sum\":[0]*len(missing_topic_ids)\n",
    "                                  ,\"count\":[0]*len(missing_topic_ids)})\n",
    "        topic_var = pd.concat([topic_var, missings])\n",
    "\n",
    "    # Calculating reduction in variance\n",
    "    topic_var[\"variance\"] = postrr_var(topic_var[\"sum\"], topic_var[\"count\"])\n",
    "    topic_var[\"p\"] = postrr_p(topic_var[\"sum\"], topic_var[\"count\"])\n",
    "    topic_var[\"var0\"] = postrr_var(topic_var[\"sum\"], topic_var[\"count\"]+1)\n",
    "    topic_var[\"var1\"] = postrr_var(topic_var[\"sum\"]+1, topic_var[\"count\"]+1)\n",
    "    topic_var[\"expected_var\"] = topic_var[\"p\"]*topic_var[\"var1\"]+(1-topic_var[\"p\"])*topic_var[\"var0\"]\n",
    "    topic_var[\"var_diff\"] = topic_var[\"expected_var\"]-topic_var[\"variance\"]\n",
    "\n",
    "    # Allocating topics\n",
    "    allocated_topics = topic_var.sort_values(\"var_diff\", ascending=True)[:scores_per_r][\"sim_topic_id\"]\n",
    "    total_scores -= scores_per_r\n",
    "\n",
    "    selected_scores = trial_sim_scores[(trial_sim_scores[\"sim_rater_id\"]==sim_rater_id)&\n",
    "                        (trial_sim_scores[\"sim_topic_id\"].isin(allocated_topics))]\n",
    "    scores = pd.concat([scores, selected_scores])\n",
    "    \n",
    "sim_scores = pd.concat([sim_scores, scores], axis=\"index\", ignore_index=True)\n",
    "sim_scores.to_csv(f\"data/{sim_name}/score_{sim_id}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_topic_measures",
   "language": "python",
   "name": "bayes_topic_measures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
