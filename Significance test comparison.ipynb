{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14376ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "/home/roger/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/roger/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/roger/.local/lib/python3.10/site-packages/pymc/sampling_jax.py:37: UserWarning: This module is experimental.\n",
      "  warnings.warn(\"This module is experimental.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.special import logit, expit\n",
    "from scipy.stats import uniform, norm, bernoulli, mannwhitneyu\n",
    "from matplotlib import pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from modeltools import mcmc_diagnostics, create_summary_stat\n",
    "from downcast import downcast_df\n",
    "from simulatescores import simulate_scores\n",
    "import jax\n",
    "from pymc.sampling_jax import sample_numpyro_nuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b19040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setting up seeds\n",
    "seed = 42\n",
    "\n",
    "# Setting numpy seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# GPU setting\n",
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810fe09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StreamExecutorGpuDevice(id=0, process_index=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19915439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['simulatescores'])\n",
    "from simulatescores import simulate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e5bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/unit_level_ratings.csv\",index_col = 0)\n",
    "raw_data = raw_data.sort_values(by=[\"corpus\", \"model\", \"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa919f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating identifier for each corpus, model, and topic\n",
    "# Identifier is unique for topic \n",
    "corpus_ids = (raw_data.groupby([\"corpus\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "corpus_ids[\"corpus_id\"] = corpus_ids.index\n",
    "\n",
    "model_ids = (raw_data.groupby([\"model\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "model_ids[\"model_id\"] = model_ids.index\n",
    "\n",
    "cordel_ids = (raw_data.groupby([\"corpus\", \"model\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "cordel_ids[\"cordel_id\"] = cordel_ids.index \n",
    "\n",
    "topic_ids = (raw_data.groupby([\"corpus\", \"model\", \"topic\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "topic_ids[\"topic_id\"] = topic_ids[\"topic\"].astype(np.int16)\n",
    "\n",
    "rater_ids = (raw_data.groupby([\"corpus\", \"rater\"], as_index=False)\n",
    "    .agg({\"intrusion\":\"count\"})\n",
    "    .drop(columns=\"intrusion\"))\n",
    "rater_ids[\"rater_id\"] = rater_ids.index \n",
    "\n",
    "\n",
    "d1 = pd.merge(raw_data, corpus_ids, on=[\"corpus\"], how=\"left\")\n",
    "d2 = pd.merge(d1, model_ids, on=[\"model\"], how=\"left\")\n",
    "d3 = pd.merge(d2, cordel_ids, on=[\"corpus\",\"model\"], how=\"left\")\n",
    "d4 = pd.merge(d3, rater_ids, on=[\"corpus\", \"rater\"], how=\"left\")\n",
    "data = pd.merge(d4, topic_ids, on=[\"corpus\", \"model\", \"topic\"], how=\"left\")\n",
    "data = data[[\"corpus_id\", \"model_id\", \"cordel_id\", \"topic_id\", \"rater_id\", \"intrusion\", \"confidence\"]]\n",
    "data, na_s = downcast_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186e467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up numpy arrays for pymc\n",
    "corpus_array = np.array(data[\"corpus_id\"])\n",
    "n_corpora = data[\"corpus_id\"].nunique()\n",
    "\n",
    "model_array = np.array(data[\"model_id\"])\n",
    "n_models = data[\"model_id\"].nunique()\n",
    "\n",
    "cordel_array = np.array(data[\"cordel_id\"])\n",
    "n_cordels = data[\"cordel_id\"].nunique()\n",
    "\n",
    "topic_array = np.array([data[\"cordel_id\"], data[\"topic_id\"]])\n",
    "n_topics = data[\"topic_id\"].nunique()\n",
    "\n",
    "rater_array = np.array(data[\"rater_id\"])\n",
    "n_raters = data[\"rater_id\"].nunique()\n",
    "\n",
    "score_array = np.array(data[\"intrusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c4e98",
   "metadata": {},
   "source": [
    "## Bayesian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17063a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and MCMC specifications\n",
    "\n",
    "n_cores = 2\n",
    "empirical_mean = logit(0.75)\n",
    "r_lambda = 2\n",
    "t_lambda = 1\n",
    "t_sigma = 1\n",
    "# cm_lambda = 2\n",
    "# cm_sigma = 1\n",
    "mu_sigma = 1\n",
    "\n",
    "glm_rater_topic_cordel = {\"model\":pm.Model()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b2d62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/.local/lib/python3.10/site-packages/pymc/sampling_jax.py:548: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  pmap_numpyro = MCMC(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time =  0:00:01.366462\n",
      "Sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█| 2000/2000 [00:41<00:00, 47.96it/s, 15 steps of size 2.12e-01. ac\n",
      "sample: 100%|█| 2000/2000 [00:56<00:00, 35.67it/s, 63 steps of size 1.92e-01. ac\n",
      "sample: 100%|█| 2000/2000 [00:54<00:00, 36.62it/s, 31 steps of size 1.88e-01. ac\n",
      "sample: 100%|█| 2000/2000 [00:58<00:00, 34.19it/s, 63 steps of size 1.93e-01. ac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling time =  0:03:31.396917\n",
      "Transforming variables...\n",
      "Transformation time =  0:00:00.057619\n",
      "Computing Log Likelihood...\n",
      "Log Likelihood time =  0:00:00.196473\n"
     ]
    }
   ],
   "source": [
    "# Rater, Topic, Cordel model\n",
    "\n",
    "glm_rater_topic_cordel[\"model\"] = pm.Model()\n",
    "with glm_rater_topic_cordel[\"model\"]:\n",
    "    # Hyperparameter priors\n",
    "    raters = pm.Data(\"raters\", rater_array, mutable=True, dims=\"obs_id\")\n",
    "    topics = pm.Data(\"topics\", topic_array, mutable=True, dims=[\"cordel\", \"topic\"])\n",
    "    cordels = pm.Data(\"cordels\", cordel_array, mutable=True, dims=\"obs_id\")\n",
    "    \n",
    "    sigma_r = pm.Exponential(\"sigma_r\", lam=r_lambda)\n",
    "    zr = pm.Normal(\"zr\",mu=0, sigma=1, shape=n_raters)\n",
    "    sigma_a = pm.Exponential(\"sigma_a\", lam=t_lambda)\n",
    "    za = pm.Normal(\"za\",mu=0, sigma=t_sigma, shape=(n_cordels, n_topics)) \n",
    "    mu = pm.Normal(\"mu\",mu=empirical_mean, sigma=mu_sigma, shape=n_cordels)\n",
    "    \n",
    "    s = pm.Bernoulli(\n",
    "            \"s\", \n",
    "            p=pm.math.invlogit(\n",
    "                mu[cordels]+\n",
    "                za[topics[0],topics[1]]*sigma_a+\n",
    "                zr[raters]*sigma_r),\n",
    "            observed=score_array, \n",
    "            dims=\"obs_id\")\n",
    "\n",
    "    c_mean = pm.Deterministic(\"c_mean\", \n",
    "                              pm.math.invlogit(mu + (za.T*sigma_a)).mean(axis=0), \n",
    "                              dims=\"obs_id\")\n",
    "    \n",
    "    if GPU:\n",
    "        glm_rater_topic_cordel[\"trace\"]=sample_numpyro_nuts(random_seed=seed)#, chain_method=\"vectorized\")\n",
    "    else:\n",
    "        glm_rater_topic_cordel[\"trace\"]=pm.sample(cores=n_cores, random_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_rater_topic_cordel[\"summary_stat\"] = create_summary_stat(glm_rater_topic_cordel[\"trace\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00823d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_raters(n_raters=None, scores_per_rater=None, total_scores=None):\n",
    "    count_none = sum([n_raters ==None, scores_per_rater ==None, total_scores ==None])\n",
    "    assert(count_none == 2)\n",
    "    \n",
    "    if n_rater == None:\n",
    "        n_raters = total_scores//scores_per_rater + 1\n",
    "    \n",
    "    if scores_per_rater == None:\n",
    "        scores_per_rater = total_scores//n_raters + 1\n",
    "    \n",
    "    if total_scores == None:\n",
    "        total_scores = n_raters*scores_per_rater\n",
    "    \n",
    "    return n_raters, scores_per_rater, total_scores\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utest_pval(scores):\n",
    "    # Utests whether the two distributions in the scores are statisticaly significant\n",
    "    # returns 1 if cordel 1 intrusion scores is statistically higher\n",
    "    # returns 0 if no statistical difference\n",
    "    # returns -1 if cordel 2 intrusion scores are statistically higher \n",
    "\n",
    "    sums = scores.groupby([\"sim_cordel_id\", \"sim_topic_id\"]).agg({\"intrusion\":\"sum\"}).reset_index()\n",
    "    cordel0_intrusions = sums[sums[\"sim_cordel_id\"]==0][\"intrusion\"]\n",
    "    cordel1_intrusions = sums[sums[\"sim_cordel_id\"]==1][\"intrusion\"]\n",
    "    p_val = mannwhitneyu(cordel0_intrusions, cordel1_intrusions, alternative=\"two-sided\")[1]\n",
    "    \n",
    "    return p_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a967029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bht_pval(sample):\n",
    "# sample = scores[scores[\"sim_id\"]==0]\n",
    "\n",
    "# Bayesian hypothesis tests whether the two distributions in the sample are statisticaly significant\n",
    "# Setting up numpy arrays for pymc\n",
    "# Only 2 models and 1 corpus in simulation\n",
    "    corpus_array = np.array([0]*len(sample))\n",
    "    n_corpora = 1\n",
    "\n",
    "    model_array = np.array(sample[\"sim_cordel_id\"])\n",
    "    n_models = sample[\"sim_cordel_id\"].nunique()\n",
    "\n",
    "    cordel_array = np.array(sample[\"sim_cordel_id\"])\n",
    "    n_cordels = sample[\"sim_cordel_id\"].nunique()\n",
    "\n",
    "    topic_array = np.array([sample[\"sim_cordel_id\"], sample[\"sim_topic_id\"]])\n",
    "    n_topics = sample[\"sim_topic_id\"].nunique()\n",
    "\n",
    "    rater_array = np.array(sample[\"sim_rater_id\"])\n",
    "    n_raters = sample[\"sim_rater_id\"].nunique()\n",
    "\n",
    "    score_array = np.array(sample[\"intrusion\"])\n",
    "\n",
    "\n",
    "    # Model and MCMC specifications\n",
    "    n_cores = 2\n",
    "    empirical_mean = logit(0.75)\n",
    "    r_lambda = 2\n",
    "    t_lambda = 1\n",
    "    t_sigma = 1\n",
    "    # cm_lambda = 2\n",
    "    # cm_sigma = 1\n",
    "    mu_sigma = 1\n",
    "\n",
    "    glm = {\"model\":pm.Model()}\n",
    "\n",
    "    # Rater, Topic, Cordel model\n",
    "\n",
    "    glm[\"model\"] = pm.Model()\n",
    "    with glm[\"model\"]:\n",
    "        # Hyperparameter priors\n",
    "        raters = pm.Data(\"raters\", rater_array, mutable=True, dims=\"obs_id\")\n",
    "        topics = pm.Data(\"topics\", topic_array, mutable=True, dims=[\"cordel\", \"topic\"])\n",
    "        cordels = pm.Data(\"cordels\", cordel_array, mutable=True, dims=\"obs_id\")\n",
    "\n",
    "        sigma_r = pm.Exponential(\"sigma_r\", lam=r_lambda)\n",
    "        zr = pm.Normal(\"zr\",mu=0, sigma=1, shape=n_raters)\n",
    "        sigma_a = pm.Exponential(\"sigma_a\", lam=t_lambda)\n",
    "        za = pm.Normal(\"za\",mu=0, sigma=t_sigma, shape=(n_cordels, n_topics)) \n",
    "        mu = pm.Normal(\"mu\",mu=empirical_mean, sigma=mu_sigma, shape=n_cordels)\n",
    "\n",
    "        s = pm.Bernoulli(\n",
    "                \"s\", \n",
    "                p=pm.math.invlogit(\n",
    "                    mu[cordels]+\n",
    "                    za[topics[0],topics[1]]*sigma_a+\n",
    "                    zr[raters]*sigma_r),\n",
    "                observed=score_array, \n",
    "                dims=\"obs_id\")\n",
    "\n",
    "        c_mean = pm.Deterministic(\"c_mean\", \n",
    "                                  pm.math.invlogit(mu + (za.T*sigma_a)).mean(axis=0), \n",
    "                                  dims=\"obs_id\")\n",
    "        c_diff = pm.Deterministic(\"c_diff\", c_mean.reshape([n_cordels,1]) - c_mean.reshape([1,n_cordels]), dims=\"obs_id\")\n",
    "\n",
    "        glm[\"trace\"]=pm.sample(cores=n_cores, progressbar=False)\n",
    "\n",
    "    n_negatives = (glm[\"trace\"].posterior[\"c_diff\"].sel({\"obs_id\":1, \"c_diff_dim_1\":0}) < 0).sum().item()\n",
    "    \n",
    "    return  n_negatives/len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf11ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sim_settings(n_trials, p_diff, n_raters=None, scores_per_r=None, total_scores=None):\n",
    "#     n_trials = 10\n",
    "#     p_diff = (0.02, 0.2)\n",
    "#     n_raters = (20, 100)\n",
    "#     scores_per_r = 40\n",
    "#     total_scores = 1300\n",
    "    # Checking only 2 of 3 score and rater variables is declared\n",
    "    count_none = sum([n_raters == None, scores_per_r == None, total_scores == None])\n",
    "    assert count_none == 1, \"There should be 2 score/rater variables declared\"\n",
    "\n",
    "    # Sampling uniform random variables\n",
    "    if type(p_diff) == tuple:\n",
    "        col_p_diff = uniform.rvs(loc = p_diff[0], scale=p_diff[1]-p_diff[0], size=n_trials)\n",
    "    else:\n",
    "        col_p_diff = np.array([p_diff]*n_trials)\n",
    "\n",
    "    if type(n_raters) == tuple:\n",
    "        col_n_raters = np.random.randint(n_raters[0], high=n_raters[1], size=n_trials)\n",
    "    else:\n",
    "        col_n_raters = np.array([n_raters]*n_trials)\n",
    "\n",
    "    if type(scores_per_r) == tuple:\n",
    "        col_scores_per_r = np.random.randint(scores_per_r[0], high=scores_per_r[1], size=n_trials)\n",
    "    else:\n",
    "        col_scores_per_r = np.array([scores_per_r]*n_trials)\n",
    "    \n",
    "    if type(total_scores) == tuple:\n",
    "        col_total_scores = np.random.randint(total_scores[0], high=total_scores[1], size=n_trials)\n",
    "    else:\n",
    "        col_total_scores = np.array([total_scores]*n_trials)\n",
    "\n",
    "    # Calculating remaining column\n",
    "    if n_raters == None:\n",
    "        col_n_raters = (col_total_scores-1)//col_scores_per_r + 1\n",
    "    elif scores_per_r == None:\n",
    "        col_scores_per_r = (col_total_scores-1)//col_n_raters + 1\n",
    "    elif total_scores == None:\n",
    "        col_total_scores = col_scores_per_r * col_n_raters\n",
    "    else:\n",
    "        raise Exception(\"How did you even get this exception? Should've been impossible, but congratulations\")\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        np.array([col_p_diff, col_n_raters, col_scores_per_r, col_total_scores]).T,\n",
    "        columns=[\"p_diff\", \"n_raters\", \"scores_per_r\", \"total_scores\"])\n",
    "    \n",
    "    df = df.astype({\n",
    "        \"p_diff\":float,\n",
    "        \"n_raters\":np.uint16,\n",
    "        \"scores_per_r\":np.uint16,\n",
    "        \"total_scores\":np.uint16\n",
    "    })\n",
    "\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sig_tests(n_trials, p_diff, n_raters=None, scores_per_r=None, total_scores=None, sims_per_trial=1):\n",
    "#     p_diff = (0.02, 0.2)\n",
    "#     n_raters = (20, 100)\n",
    "#     scores_per_r = 40\n",
    "#     sims_per_trial = 2\n",
    "#     n_trials = 5\n",
    "\n",
    "    # Checking only 2 of 3 score and rater variables is declared\n",
    "    count_none = sum([n_raters == None, scores_per_r == None, total_scores == None])\n",
    "    assert(count_none == 1)\n",
    "\n",
    "\n",
    "\n",
    "    sim_results = pd.DataFrame(\n",
    "        columns=[\"sim_id\", \"p_diff\", \"n_raters\", \"scores_per_r\", \"total_scores\", \"utest_pval\", \"bht_pval\"]\n",
    "    )\n",
    "    sim_results = sim_results.astype({\n",
    "        \"p_diff\":float,\n",
    "        \"n_raters\":np.uint16,\n",
    "        \"scores_per_r\":np.uint16, \n",
    "        \"total_scores\":np.uint16,\n",
    "        \"sim_id\":int,\n",
    "        \"utest_pval\":float, \n",
    "        \"bht_pval\":float,\n",
    "    })\n",
    "\n",
    "    settings_df = generate_sim_settings(n_trials=n_trials, p_diff=p_diff, n_raters=n_raters, \n",
    "                                        scores_per_r=scores_per_r, total_scores=total_scores)\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        p_diff = settings_df.loc[i, \"p_diff\"].item()\n",
    "        n_raters = settings_df.loc[i, \"n_raters\"].item()\n",
    "        scores_per_r = settings_df.loc[i, \"scores_per_r\"].item()\n",
    "\n",
    "        scores = simulate_scores(\n",
    "            glm_rater_topic_cordel,\n",
    "            p_diff=p_diff,\n",
    "            n_raters=n_raters,\n",
    "            scores_per_r=scores_per_r,\n",
    "            n_sims=sims_per_trial)\n",
    "\n",
    "        for j in range(sims_per_trial):\n",
    "            sim_results = pd.concat([sim_results, pd.DataFrame(\n",
    "                [[p_diff, n_raters, scores_per_r, j,\n",
    "                 utest_pval(scores[scores[\"sim_id\"]==j]),\n",
    "                 bht_pval(scores[scores[\"sim_id\"]==j])]],\n",
    "                columns=[\"p_diff\", \"n_raters\", \"scores_per_r\", \"sim_id\", \"utest_pval\", \"bht_pval\"]\n",
    "            )], ignore_index=True)\n",
    "\n",
    "    return sim_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de9203",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdc014d4",
   "metadata": {},
   "source": [
    "scatter. colour is two significance tests\n",
    "p values vs n_raters\n",
    "p values vs scores_per_r (holding number of total ratings constant)\n",
    "\n",
    "n_raters vs p_diff\n",
    "\n",
    "\n",
    "Settings used in hoyle's significance simulation:\n",
    "Total ratings: 50 topics * 26 ratings per topic\n",
    "\n",
    "Topic quality diff \n",
    "- Better model (x+4)*0.85 + 50-(x+4)*(1/6)\n",
    "- Worse model x*0.85 + 50-x*(1/6)\n",
    "Average topic difference = 0.054666 or 0.55\n",
    "\n",
    "Settings used in Hoyle's actual surveys\n",
    "n_raters = 38\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c74a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for hoyle's significance testing\n",
    "hoyle_total_scores = 50*26\n",
    "hoyle_p_diff = 0.055\n",
    "hoyle_n_raters = 38\n",
    "hoyle_scores_per_r = hoyle_total_scores//hoyle_n_raters + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c50ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total ratings between 400 and 2400\n",
    "# Hoyle used 1300 total ratings\n",
    "sim_n_raters = simulate_sig_tests(\n",
    "    p_diff = hoyle_p_diff,\n",
    "    n_raters = (10, 60),\n",
    "    scores_per_r = hoyle_scores_per_r,\n",
    "    sims_per_trial = 1,\n",
    "    n_trials = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdefc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_scores_per_r = simulate_sig_tests(\n",
    "    p_diff = hoyle_p_diff,\n",
    "    scores_per_r = (10, 100),\n",
    "    total_scores = hoyle_total_scores,\n",
    "    sims_per_trial = 1,\n",
    "    n_trials = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0498d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total ratings between 400 and 2400\n",
    "# Hoyle used 1300 total ratings\n",
    "sim_p_n_raters = simulate_sig_tests(\n",
    "    p_diff = (0.02, 0.2),\n",
    "    n_raters = (10, 60),\n",
    "    scores_per_r = hoyle_scores_per_r,\n",
    "    sims_per_trial = 1,\n",
    "    n_trials = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_p_scores_per_r = simulate_sig_tests(\n",
    "    p_diff = (0.02, 0.2),\n",
    "    scores_per_r = (10, 100),\n",
    "    total_scores = hoyle_total_scores,\n",
    "    sims_per_trial = 1,\n",
    "    n_trials = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d0604",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef724729",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_results[\"utest\"] = sim_results[\"utest_pval\"] < alpha\n",
    "sim_results[\"bht\"] = sim_results[\"bht_pval\"] < alpha\n",
    "\n",
    "# (utest, bht)\n",
    "# Neither significant: orange\n",
    "# bht significant: yellow\n",
    "# utest significant: blue\n",
    "# both significant: green\n",
    "\n",
    "sim_results[\"c\"] = np.select(\n",
    "    condlist=[\n",
    "        ~sim_results[\"utest\"] & ~sim_results[\"bht\"],\n",
    "        ~sim_results[\"utest\"] & sim_results[\"bht\"],\n",
    "        sim_results[\"utest\"] & ~sim_results[\"bht\"],\n",
    "        sim_results[\"utest\"] & sim_results[\"bht\"]\n",
    "    ],\n",
    "    choicelist=[\"orangered\", \"yellow\", \"blue\", \"green\"],\n",
    "    default=\"black\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f76973",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_raters_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1539f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(n_raters_results[\"p_diff\"], n_raters_results[\"n_raters\"], c=n_raters_results[\"c\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16041a72",
   "metadata": {},
   "source": [
    ". \\\n",
    ". \\\n",
    ". \\\n",
    ". \\\n",
    ". \\\n",
    ". \\\n",
    ". \\\n",
    ". \n",
    "## Checking score simulation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating scores\n",
    "p_diff = 0.08\n",
    "n_raters = 40\n",
    "scores_per_r = 40\n",
    "n_sims = 1_000\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = simulate_scores(\n",
    "    glm_rater_topic_cordel,\n",
    "    p_diff=p_diff,\n",
    "    n_raters=n_raters,\n",
    "    scores_per_r=scores_per_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores = scores.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking scores\n",
    "\n",
    "_, ax = plt.subplots(2,1,figsize=(10,10))\n",
    "\n",
    "n_ratings_per_cordel = n_raters*scores_per_r/2\n",
    "\n",
    "sns.kdeplot(sim_scores[sim_scores[\"sim_cordel_id\"]==0].groupby([\"sim_id\"]).agg({\"intrusion\":\"sum\"})[\"intrusion\"]/n_ratings_per_cordel, ax=ax[0])\n",
    "sns.kdeplot(sim_scores[sim_scores[\"sim_cordel_id\"]==1].groupby([\"sim_id\"]).agg({\"intrusion\":\"sum\"})[\"intrusion\"]/n_ratings_per_cordel, ax=ax[0])\n",
    "ax[0].legend(ax[0].get_lines(), [\"Cordel 0 means\", \"Cordel 1 means\"])\n",
    "ax[0].set_xlabel(\"Cordel means (probability)\")\n",
    "ax[0].set_title(f\"Simulated means by cordel\")\n",
    "\n",
    "diff = sim_scores[sim_scores[\"sim_cordel_id\"]==1].groupby([\"sim_id\"]).agg({\"intrusion\":\"sum\"}) \\\n",
    "        - sim_scores[sim_scores[\"sim_cordel_id\"]==0].groupby([\"sim_id\"]).agg({\"intrusion\":\"sum\"})\n",
    "\n",
    "sns.kdeplot(diff[\"intrusion\"]/n_ratings_per_cordel, ax=ax[1])\n",
    "ax[1].set_title(f\"Difference in cordel means\")\n",
    "plt.plot()\n",
    "\n",
    "print(f\"Perc of simulations with mean1 < mean0 (n raters={n_raters}): {(diff['intrusion']<=0).sum()/1000:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "size=100\n",
    "\n",
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "\n",
    "\n",
    "# Selecting 100 random simulations\n",
    "rand_ids = np.random.choice(range(1000), size=size, replace=False)\n",
    "\n",
    "# Calculating topic proportions\n",
    "samples = (sim_scores[sim_scores[\"sim_id\"].isin(rand_ids)].groupby([\"sim_id\", \"sim_topic_id\"])\n",
    "           .agg({\"intrusion\":[\"sum\", \"count\"]}).reset_index())\n",
    "samples[\"p\"] = samples[(\"intrusion\", \"sum\")]/samples[(\"intrusion\", \"count\")]\n",
    "# Plotting proportions\n",
    "for sim_id in rand_ids:\n",
    "    sns.kdeplot(samples[samples[\"sim_id\"]==sim_id][\"p\"], ax=ax, color=\"navy\", alpha=7/size**0.9)\n",
    "\n",
    "# Repeat calculations and plots for actual data\n",
    "actuals = (data.groupby([\"cordel_id\", \"topic_id\"])\n",
    "           .agg({\"intrusion\":[\"sum\", \"count\"]}).reset_index())\n",
    "actuals[\"p\"]=actuals[(\"intrusion\", \"sum\")]/actuals[(\"intrusion\", \"count\")]\n",
    "for cordel_id in range(6):\n",
    "    sns.kdeplot(actuals[actuals[\"cordel_id\"]==cordel_id][\"p\"], ax=ax, color=\"orange\", alpha=0.7)\n",
    "\n",
    "ax.set_title(\"Simulated topic probabilities (blue) vs actual topic probabilities (orange)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677ed36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size=100\n",
    "\n",
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "\n",
    "\n",
    "# Selecting 100 random simulations\n",
    "rand_ids = np.random.choice(range(1000), size=size, replace=False)\n",
    "\n",
    "# Calculating rater proportions\n",
    "samples = (sim_scores[sim_scores[\"sim_id\"].isin(rand_ids)].groupby([\"sim_id\", \"sim_rater_id\"])\n",
    "           .agg({\"intrusion\":[\"sum\", \"count\"]}).reset_index())\n",
    "samples[\"p\"] = samples[(\"intrusion\", \"sum\")]/samples[(\"intrusion\", \"count\")]\n",
    "# Plotting proportions\n",
    "for sim_id in rand_ids:\n",
    "    sns.kdeplot(samples[samples[\"sim_id\"]==sim_id][\"p\"], ax=ax, color=\"navy\", alpha=0.05)\n",
    "\n",
    "# Repeat calculations and plots for actual data\n",
    "actuals = (data.groupby([\"corpus_id\", \"rater_id\"])\n",
    "           .agg({\"intrusion\":[\"sum\", \"count\"]}).reset_index())\n",
    "actuals[\"p\"]=actuals[(\"intrusion\", \"sum\")]/actuals[(\"intrusion\", \"count\")]\n",
    "for corpus_id in range(2):\n",
    "    sns.kdeplot(actuals[actuals[\"corpus_id\"]==corpus_id][\"p\"], ax=ax, color=\"orange\", alpha=1)\n",
    "\n",
    "ax.set_title(\"Simulated rater probabilities (blue) vs actual rater probabilities (orange)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes_topic_measures",
   "language": "python",
   "name": "bayes_topic_measures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
